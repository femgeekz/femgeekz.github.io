---
layout: post
title: Monitoring S3 uploads for a real time data
date: '2013-04-15T22:22:00.000+05:30'
author: Swathi Venkatachala
tags:
- s3sync
- ubuntu
- s3cmd
- uploads
- s3
- monitoring
- real time data
- upload to s3
- automation
- monitor files
- watcher
modified_time: '2013-04-18T03:05:15.724+05:30'
blogger_id: tag:blogger.com,1999:blog-6187879032785791598.post-5795697084558914906
blogger_orig_url: http://femgeekz.blogspot.com/2013/04/monitoring-s3-uploads-for-real-time-data.html
comments: true
---

<div dir="ltr" style="text-align: left;" trbidi="on"><span style="font-family: Trebuchet MS, sans-serif;">&nbsp; &nbsp; &nbsp; &nbsp; If you are working on Big Data and its bleeding edge technologies like Hadoop etc., the primary thing you need is a "dataset" to work on. So, this data can be reviews, blogs, news, social media data (Twitter, Facebook etc), domain specific data, research data, forums, groups, feeds, fire hose data etc. Generally, companies reach the data vendors to fetch such kind of data.</span><br /><span style="font-family: Trebuchet MS, sans-serif;"><br /></span><span style="font-family: Trebuchet MS, sans-serif;">&nbsp; &nbsp; &nbsp; &nbsp; Normally, these data vendors dump the data into a shared server kind of environment. For us to use this data for processing with MapReduce and so forth, we move them to S3 for storage first and processing next. Assume, the data belong to social media such as Twitter or Facebook, then the data can be dumped according to the date format directory. Majority of the cases, its the&nbsp;practice.</span><br /><span style="font-family: Trebuchet MS, sans-serif;">Also assuming 140-150GB /day being dumped in a hierarchy like 2013/04/15 ie. yyyy/mm/dd format, stream of data, how do you&nbsp;</span><br /><span style="font-family: Trebuchet MS, sans-serif;">- &nbsp;upload them to s3 in the same hierarchy to a given bucket?</span><br /><span style="font-family: Trebuchet MS, sans-serif;">- &nbsp;monitor the new incoming files and upload them?</span><br /><span style="font-family: Trebuchet MS, sans-serif;">- &nbsp;save the space effectively on the disk?</span><br /><span style="font-family: Trebuchet MS, sans-serif;">- &nbsp;ensure the reliability of uploads to s3?</span><br /><span style="font-family: Trebuchet MS, sans-serif;">- &nbsp;clean if the logging is enabled to track?</span><br /><span style="font-family: Trebuchet MS, sans-serif;">- &nbsp;re-try the failed uploads?</span><br /><span style="font-family: Trebuchet MS, sans-serif;"><br /></span><span style="font-family: Trebuchet MS, sans-serif;">These were some of the questions, running at the back of my mind, when I wanted to automate the uploads to S3. Also, I wanted 0 human intervention or&nbsp;at-least&nbsp;the least!</span><br /><span style="font-family: Trebuchet MS, sans-serif;">So, I came up with&nbsp;</span><br /><span style="font-family: Trebuchet MS, sans-serif;">- s3sync / s3cmd.</span><br /><span style="font-family: Trebuchet MS, sans-serif;">- the python Watcher script by&nbsp;</span><span style="font-family: 'Trebuchet MS', sans-serif;">Greggory Hernandez, here </span><a href="https://github.com/greggoryhz/Watcher" style="font-family: 'Trebuchet MS', sans-serif;">https://github.com/greggoryhz/Watcher</a><span style="font-family: 'Trebuchet MS', sans-serif;">&nbsp;</span><br /><span style="font-family: 'Trebuchet MS', sans-serif;">A big thanks! This helped me with monitoring part and it works so great!</span><br /><span style="font-family: 'Trebuchet MS', sans-serif;">- few of my own scripts.</span><br /><div><span style="font-family: Trebuchet MS, sans-serif;"><br /></span><span style="font-family: Trebuchet MS, sans-serif;">What are the ingredients?</span><br /><ul style="text-align: left;"><li><span style="font-family: 'Trebuchet MS', sans-serif;">&nbsp;Installation of s3sync. I have just used one script of s3cmd here and not s3sync in real. May be in future -- so I have this.</span></li><script src="https://gist.github.com/SwathiMystery/5388392.js"></script><li><span style="font-family: Trebuchet MS, sans-serif;">Installation of Watcher.</span></li><script src="https://gist.github.com/SwathiMystery/5388464.js"></script><li><span style="font-family: Trebuchet MS, sans-serif;">My own wrapper scripts.</span></li><li><span style="font-family: Trebuchet MS, sans-serif;">cron</span></li></ul><div><span style="font-family: Trebuchet MS, sans-serif;">Next, having set up of the environment ready, lets make some common "assumptions".</span></div><div><ul style="text-align: left;"><li><span style="font-family: Trebuchet MS, sans-serif;">Data being dumped will be at /home/ubuntu/data/ -- from there it could be 2013/04/15 for ex.</span></li><li><span style="font-family: Trebuchet MS, sans-serif;">s3sync is located at /home/ubuntu</span></li><li><span style="font-family: Trebuchet MS, sans-serif;">Watcher repository is at /home/ubuntu</span></li></ul><div><span style="font-family: Trebuchet MS, sans-serif;">Getting our hands dirty...</span></div><div><ul style="text-align: left;"><li><span style="font-family: 'Trebuchet MS', sans-serif;">Goto Watcher and set the directory to be watched for and corresponding action to be undertaken.</span></li><script src="https://gist.github.com/SwathiMystery/5388835.js"></script><li><span style="font-family: 'Trebuchet MS', sans-serif;">Create a script called monitor.sh to upload to s3 in s3sync directory as below.</span></li><ul><li><span style="font-family: 'Trebuchet MS', sans-serif;">The variables you may like to change is s3bucket path in "s3path" in monitor.sh</span></li><li><span style="font-family: 'Trebuchet MS', sans-serif;">This script will upload the new incoming file detected by the watcher script in the reduced redundancy storage format. (you can remove the header -- provided you are not interested to store in RRS format)</span></li><li><span style="font-family: 'Trebuchet MS', sans-serif;">The script will call s3cmd ruby script to upload recursively and thus maintains the hierarchy ie. yyyy/mm/dd format with files *.*</span></li><li><span style="font-family: 'Trebuchet MS', sans-serif;">It will delete the file successfully uploaded to s3 from the local path -- to save the disk space.</span></li><li><span style="font-family: Trebuchet MS, sans-serif;">The script would not delete the directory, as it will be taken care by yet another script re-upload.sh, which acts as a backup for the failed uploads to be uploaded again to s3.</span></li></ul><script src="https://gist.github.com/SwathiMystery/5389095.js"></script><li><span style="font-family: 'Trebuchet MS', sans-serif;">Create a script called re-upload.sh which will upload the failed file uploads.</span></li><ul><li><span style="font-family: Trebuchet MS, sans-serif;">This script ensures that the files that are left over from monitor.sh (failed uploads -- this chance is very less. May be 2-4 files/day. -- due to various reasons.), will be uploaded to s3 again with the same hierarchy in RRS format.</span></li><li><span style="font-family: Trebuchet MS, sans-serif;">&nbsp;Post successful upload, deletes the file and hence the directory if empty.</span></li></ul><script src="https://gist.github.com/SwathiMystery/5389191.js"></script><li><span style="font-family: 'Trebuchet MS', sans-serif;">Now, more dirtiest work -- Logging and cleaning logs.</span></li><ul><li><span style="font-family: Trebuchet MS, sans-serif;">All the "echo" created in monitor.sh can be found in ~/.watcher/watcher.log when the watcher.py is running.</span></li><li><span style="font-family: Trebuchet MS, sans-serif;">This log helps us initially and may be later too, to backtrack errors or so.</span></li><li><span style="font-family: Trebuchet MS, sans-serif;">Call of duty - Janitor for cleaning logs. To do this, we can use cron to run a script at sometime. I was interested to run - Every Saturday at 8.00 AM</span></li><li><span style="font-family: Trebuchet MS, sans-serif;">Create a script to clean log as "clean_log.sh" in /home/ubuntu/s3sync</span></li></ul><li><span style="font-family: Trebuchet MS, sans-serif;">Time for cron</span></li><script src="https://gist.github.com/SwathiMystery/5389305.js"></script><ul><li><span style="font-family: Trebuchet MS, sans-serif;">All set! logging clean happens every Saturday 8.00 AM and re-upload script runs for the previous day, to check if files exist and does the cleaning accordingly.</span></li></ul><li><span style="font-family: Trebuchet MS, sans-serif;">Let's start the script</span></li><script src="https://gist.github.com/SwathiMystery/5389359.js"></script> </ul><div>So, this assures successful uploads &nbsp;to S3.&nbsp;</div></div></div><div>My bash-fu with truth! ;)</div><div>Happy Learning! :)</div></div></div>